# TRUSTED AI SOC LITE

Prototype local Security Operations Center (SOC) that combines automated Nmap scans, explainable AI analytics, Wazuh SIEM integration and a realtime dashboard.

## ğŸ“¦ Project Structure

```
trusted_ai_soc_lite/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ settings.yaml
â”œâ”€â”€ scanner/
â”‚   â”œâ”€â”€ nmap_scan.py
â”‚   â””â”€â”€ parse_results.py
â”œâ”€â”€ ai_engine/
â”‚   â”œâ”€â”€ train_model.py
â”‚   â”œâ”€â”€ detect_anomalies.py
â”‚   â””â”€â”€ xai_explain.py
â”œâ”€â”€ response/
â”‚   â”œâ”€â”€ block_ip.py
â”‚   â””â”€â”€ notify.py
â”œâ”€â”€ logs/
â”‚   â”œâ”€â”€ audit.py
â”‚   â”œâ”€â”€ audit.json              # generated
â”‚   â”œâ”€â”€ wazuh_events.ndjson     # generated
â”‚   â””â”€â”€ scans/                  # generated
â”œâ”€â”€ dashboard/
â”‚   â””â”€â”€ streamlit_app.py
â”œâ”€â”€ integration/
â”‚   â””â”€â”€ wazuh/
â”‚       â”œâ”€â”€ ossec.local.conf
â”‚       â”œâ”€â”€ decoders/
â”‚       â”‚   â””â”€â”€ trusted-ai-soc_decoders.xml
â”‚       â””â”€â”€ rules/
â”‚           â””â”€â”€ trusted-ai-soc_rules.xml
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ docker-compose.yml
â”œâ”€â”€ scripts/                    # reserved for automation helpers
â”œâ”€â”€ .env.example
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Makefile
â””â”€â”€ README.md
```

## ğŸš€ Quickstart (Debian / Linux)

1. **Install prerequisites**

   ```bash
   sudo apt update && sudo apt install -y python3 python3-venv python3-pip nmap make docker.io docker-compose
   ```

2. **Clone and enter the repository**

   ```bash
   git clone <repo-url> trusted_ai_soc_lite
   cd trusted_ai_soc_lite
   ```

3. **Create the Python environment**

   ```bash
   make install
   ```

   <details>
   <summary>Installing without internet access</summary>

   1. On a machine with internet, download the required wheels:

      ```bash
      pip download -r requirements.txt -d wheels/
      ```

   2. Copy the `wheels/` directory to this project and point `make install` to it:

      ```bash
      WHEEL_DIR=./wheels make install
      ```

   The Makefile will install `pip`, `setuptools`, `wheel`, and all project dependencies from the provided directory without
   contacting PyPI.

   </details>

4. **Run a scan and train the model**

   ```bash
   make scan        # collects Nmap output (simulated if nmap is missing)
   make parse       # converts the latest scan to CSV
   make train       # trains the anomaly detection model
   make detect      # scores the parsed results and writes detections JSON
   make xai         # generates XAI explanations for latest detections
   # or run everything in one shot:
   make pipeline
   ```

5. **Launch the dashboard**

   ```bash
   make streamlit
   ```

   Access the UI at [http://localhost:8501](http://localhost:8501).

## ğŸ³ Docker Deployment

1. **Copy environment template**

   ```bash
   cp .env.example .env
   ```

2. **Start the stack**

   ```bash
   cd docker
   docker compose up --build
   ```

   - Streamlit dashboard: `http://localhost:8501`
   - Wazuh dashboard (Kibana): `https://localhost:5601`

> **Note:** The official Wazuh images require at least 8 GB of RAM. Adjust Java heap sizes in `docker-compose.yml` for constrained environments.

## ğŸ”„ Automation Workflow

1. `scanner/nmap_scan.py` collects raw vulnerability data (XML or simulated JSON).
2. `scanner/parse_results.py` converts scans into tabular features.
3. `ai_engine/train_model.py` fits an Isolation Forest on accumulated scans.
4. `ai_engine/detect_anomalies.py` produces anomaly scores, severities and audit events.
5. `ai_engine/xai_explain.py` enriches detections with SHAP-based explanations.
6. `response/block_ip.py` and `response/notify.py` execute automated defense and alerting.
7. `dashboard/streamlit_app.py` visualises detections, alerts and audit trail.

All actions are recorded through `logs/audit.py` in both JSON and NDJSON formats to feed Wazuh.

## ğŸ“‘ Wazuh Integration

- Mount `integration/wazuh` into `/var/ossec/etc/shared/trusted-ai-soc` on the manager.
- Configure `ossec.local.conf` to tail `/var/trusted-ai-soc/logs/wazuh_events.ndjson`.
- Decoders and rules included to tag anomaly and response events from the SOC.

## âš™ï¸ Configuration

- `config/settings.yaml` holds all tunables: scan targets, AI thresholds, notification backends, audit file paths.
- `.env` exposes runtime variables for containers and dashboard credentials.

## ğŸ§ª Testing the Pipeline

You can simulate data without Nmap by running:

```bash
python3 scanner/nmap_scan.py --config config/settings.yaml
python3 scanner/parse_results.py logs/scans/$(ls -t logs/scans | head -n1) --output logs/parsed.csv
python3 ai_engine/train_model.py logs/parsed.csv
python3 ai_engine/detect_anomalies.py logs/parsed.csv
python3 ai_engine/xai_explain.py logs/parsed.csv logs/explanations/$(ls -t logs/explanations/detections_*.json | head -n1)
```

The fallback simulator will create sample detections which appear in the dashboard.

## ğŸ” Security Considerations

- Run all Docker containers on an isolated network segment.
- Replace default credentials and SMTP settings before production use.
- Review generated SHAP explanations to validate XAI transparency.

## ğŸ“š Documentation & Deliverables

- **Scripts** for scanning, AI, response, dashboard and audit logging.
- **Docker assets** for reproducible deployment with Wazuh stack.
- **Audit trail** persisted in JSON & NDJSON for compliance.
- **Dashboard** replicating the provided mock (dark mode, metrics, alert stream).

## ğŸ§­ Next Steps / Extensions

- Integrate OpenVAS feeds for deeper vulnerability coverage.
- Connect to OTX or MISP for threat intelligence enrichment.
- Automate PDF reporting with weekly summaries.
- Add honeypot or deception telemetry to the pipeline.
